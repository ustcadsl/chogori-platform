[-UP-](./README.md)


# Plog
The persistence data model provided/used by Chogori Project is called Plog. It is an interface for file-append-like storage services, such as PLOG or Azure AppendBlock. In our current design, this Plog is an append-only struct. The interface does not allow random offset data modifications. Instead, it provides a single mutating operation: append(), which can only append new data.

## Term
- Plog Client:  Provides an implementation of the PLOG interface. It communicates with Plog Servers. It supports 4 operations: create, append, read, and seal.
- Plog Server: Process the Plog Client requests. It reads/appends data from/to the storage media.
- Persistence Group: A Persistence Group consists of 3 Plog Servers. When a Plog Client creates/appends/seals a plog, it will always communicate with all the Plog Servers in the same Persistence Group simultaneously in order to provide redundancy.
- Persistence Cluster: A Persistence Cluster consists of several Persistence Groups. Each Persistence Cluster is registered in the CPO with a Persistence Cluster Name. When a client initializes, it will request one Persistence Cluster from the CPO with a given Persistence Cluster Name. The client can select different Persistence Groups in this Persistence Cluster in order to provide load balance.

## Plog Id
A Plog Id is a unique identifier for a Plog. This is a small tuple that conveys `(Persistence Cluster Name, Persistence Group Name, Random Identifier)`. From this tuple, the client can locate the Plog Servers that stored that Plog.

## Operations
The Plog Service provides 4 operations: create, append, read, and seal. The Plog Server uses the current offset to verify whether a request is valid or not. If the current request is invalid, it will reject this request and return an error.
- Create: The Plog client will generate a Plog Id, and send a create request to the Plog Servers in the same Persistence Group. Each Plog Server checks whether the key already exists. If not, it will pre-allocate a fixed-size block in storage media to persist the incoming request. Then, it will initialize the current offset = 0 that associated with this block. Once the Plog Client receives the success message, it will return the generated Plog Id.
- Append: The Plog Server receives an append request contains Plog Id, current offset, and the content that needs to be persisted. If the current offset is not equal to the offset the Plog Server holds, or after appending the new content, the overall size exceeds the pre-allocated block size, the Plog Server will return an error. Otherwise, the Plog Server will append the new content in this Plog's block, then return success. The I/O size for each append operation has an upper limit, currently is set to be 2MB.
- Read: Since we only use 3 Plog Servers to provide redundancy, we are imagining the Plogs with the same Plog Id in different Plog Servers are exactly the same. So when the client sends a read request, it will only send to one Plog Server in the Persistence Cluster. The Plog Server validates the reading request and returns the corresponding response.
- Seal: The client will send the seal request to the Plog Servers with a sealed offset. If this offset is inconsistent with the current offset the Plog Server holds, it will seal this Plog with the smaller offset. Once a Plog is Sealed, the client will not be allowed to make any modifications. 

## Append Request Reassembly （TODO）
It is possible that the arriving append request will arrive at the Plog Server out of order due to bad networking conditions. Therefore, on the Plog Server-side, we should cache the out-of-order requests. Once the Plog Server receives an append request, it will first check whether the request's offset is equal to the current offset of the request Plog it holds. If they are the same, this is an in-order request, we can handle it with the normal logic. Otherwise, it is an out-of-order request, the Plog Server will first reply to the Plog Client, notify the client that this packet is out-of-order. Then, it will cache this request. The append request cache is a linked list. For each out-of-order request, we insert it into the cache, sorting with its offset. Then, the Plog Server checked the neighboring requests of the inserted request, checking whether the neighboring append requests can merge. We can merge the append requests with continuous offset. If the Plog Server receives the missing append request, it will process the requests in the cache as well. 

# Log Stream
The LogStream is a K2 client library which provides a similar interface to PlogClient (create/read/append), but without any size constraints.  Users simply create a LogStream, and can append as much (or as big) data as they need. Random reads over the entire stream are also supported. The library solves the problem of managing multiple PLOG blocks to form a single, continuous, infinite(2^64) stream. Conceptually it is a list of plogs where only the last one is unsealed and can append data. It could support different types of data services such as WAL, Indexer snapshot, etc. The log stream will provide API to read/append the data, store all the metadata information such as used plog id and their offsets. In order to handle the case that a log stream crash, we need to record these metadata as well. We store the metadata of all data services in the same partition into `Metadata Manager`, which we will describe later.

## Operations
The log stream provides the operations: create, append, read. Internally, it also has API to get list of plogs, update the list of plogs (removing some plogs for GC). When the log stream creates a new plog or seals a plog, it should persist the created plog id and the sealed plog id and offset via `Metadata Manager`.
- Create: The log stream receives no inputs. It will first create one plog: the active plog. Then, it will record the active plog id and mark this log stream to be ready to handle append and read operations.
- Append offset 
- Append without offset: This operation takes one input: `payload`. It will return the plog id to indicate which plog stores this payload, and the current offset of this plog. The log stream will first check whether the current plog will exceed the size limit if it appends the payload to the current plog. If it will not exceed the size limit, the log stream client will append the input payload to the corresponding plog and return success. Otherwise, it will first seal the old plog, record the sealed offset of the old plog, create a new plog, record the plog id of the new plog, then write the payload to the new plog. We call this process `Plog Switch`. During this process, all the upcoming write should wait until the switching process finished in order to ensure correctness.
- Read: This operation takes 3 inputs: `start plog id`, `start offset` and `size`. It is possible that the input size is too large so that the log stream will read the contents from multiple plogs. The log stream will first check how many plogs this read operation will involve by checking the vector of plogs it used. Then, it will read the requested contents from all the involved plogs asynchronously. Once it receives the response from all the plogs, it will return a vector of payloads.

## Optimization
When we switch the old plog to the new plog for the write operation, all the upcoming write requests will be blocked until the switching process finished, which is very inefficient. We used several optimization methods to improve the performance of this write operation:
- Pre-allocated Plog: When we create the log stream, we will not only create one active plog. Instead, we will also create a pre-allocated next plog which is used to perform plog switch faster. Therefore, when we do the plog switch, we can use the pre-allocated plog as the new plog immediately, instead of waiting for creating a new plog. Then, we can create the new pre-allocated plog in the background afterward.
- Make the plog switch process to be asynchronous: When we switch the old plog to the new plog, it is not necessary for us to block the upcoming write operations until the switching process finished. Instead, We can perform the writes against the new plog without waiting, but not acknowledge them until we record this switch. 

# Metadata Manager
The metadata manager is a subservice that manages the metadata of all types of data services in the same partition. Each partition will only have one metadata manager. The metadata manager holds its own log stream: metadata log stream, to store the metadata of all data services. However, we need to record the metadata of metadata log stream as well. We store it into CPO.

## Operations
The metadata manager supports 3 operations: init, write, and read_all. 
- Init: The metadata manager takes one input: `partition name`. It will create a log stream called metadata log stream, then record the active plog id of this log stream to CPO associated with the partition name, which is unique in our system.
- Append Records: The metadata manager takes three inputs: `data service name`, `metadata record type`, `metadata record content`. The `data service type` is used to identify which data service this record belongs to. The `metadata record type` indicates the type of this metadata record. Currently, we support the following types: `init record`, `incremental record`, and `full records`. The `init record` only contains the first plog id of the corresponding log stream. The `incremental record` will record the previously sealed plog id, its sealed offset and the new plog id. The `full records` will record all the plogs and their sealed offset for the corresponding log stream. The write operation will put this tuple into a payload, and write this payload to the metadata log stream directly. If the `Plog Switch` happened on this metadata log stream, the metadata manager will record the sealed plog offset and new plog id to CPO as well.
- Read ALL Records: Since each partition will only have one metadata manager service, and we stored the metadata of different types of data services into the same metadata log stream, it is hard for us to determine which part of the contents we would like to read. Instead, we will read all the data from the metadata log stream. If the current metadata manager is crashed, the new metadata manager will read the latest plog id via CPO using the metadata manager name. Then, it will read the contents from the latest plog, which contains all the previously used plog ids and their offset of the metadata log stream. After that, it will read the metadata from all these plogs. Each data service will use its own metadata to become the active data service.

## Operations
- When the metadata log stream creates a new plog, we could first write all the previously used plog ids and their offsets of this metadata log stream to the new plog. Then, we could only store the latest plog id of this metadata log stream in CPO, which can save the storage space of CPO.

# TODO
Currently, we only implement this persistence part for testing the performance of our system. So there are several points we would like to implement later
- Handle the failure cases: Currently we only plan to implement the log stream for the happy cases. If there are any unexpected errors, we will not handle them, but only throw a LogStreamException. Later, we will implement handling of the failures in the log stream.
- Currently our test case cannot test the scenario that a Metadata Manager create multiple plogs to store metadata since we used a fixed plog size limit for now. We only write the plog id and offset to the metadata plog. The size of each plog id is less than 100 bytes, while the size of offset is 4 bytes. If the size of each plog is 16MB, then each metadata plog can store the metadata of more than 100k data services' plogs. Currently our testcase cannot create such large amount of plogs due to memory limit. Therefore we cannot test this scenario. We plan to add this testcase later if we can support variable size of plogs.
- Support Chunked
